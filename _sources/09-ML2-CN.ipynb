{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbfab49f",
   "metadata": {},
   "source": [
    "# 基于预测的算法\n",
    "\n",
    "在本章的学习中，我们将介绍有监督学习当中自动使用交互协变量的方法。这些数据驱动的协变量能够更灵活地捕捉数据中的特质。但是，由于其过于复杂，目前我们对这些方法的理论研究还处在起步阶段。理论学者把它们这些数学上难以解释的计算过程称为“黑箱”。在实际应用中，如果调试得当，那么它们的预测效果会十分显著。\n",
    "{cite}`gu2018empirical` 展示了用交互项辅助金融市场预测的一系列方法。但与此同时，由于它并不符合常规的科学范式，业界人士也在当心这些方法是否是类似“炼金术”之类的伪科学。在将这些方法应用于计量分析前，我们有必要采取谨慎地态度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4142fd",
   "metadata": {},
   "source": [
    "## 回归树和装袋法\n",
    "\n",
    "有监督学习实际上就是用 $x$ 来预测 $y$。理论上讲，如果我们忽略“维度诅咒”，非参数估计(例如核回归)就可以完成这些任务。\n",
    "\n",
    "{cite}`breiman1984classification`提出了 **回归树** (regression tree)方法。 回归树背后的算法其实不难理解，本质上是递归地拆分协变量。依据的标准则是残差平方和(SSR)是否降低最多，回归树每一次拆分都将一个协变量拆分为两个虚拟变量。每一次拆分后的拟合值都用 $y_i$ 在每小块 $x$ 上的简单平均来计算。\n",
    "\n",
    "回归树的调谐参数是它的深度，也就是拆分的次数。给定一个数据集 $d$ 和回归树的深度，就能用数据拟合的回归树 $\\hat{r}(d)$ 。\n",
    "\n",
    "回归树的问题在于它的不稳定性。对于同一个数据生成过程下产生的不同样本，算法通常选择不同的协变量和分裂点，会极大地影响拆分的路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ff20c0",
   "metadata": {},
   "source": [
    "\n",
    "**自举平均法** (bootstrap averaging)，也叫 **装袋法** (bagging) 可以用于减少回归树的方差[{cite}`breiman1996bagging`]。 装袋法在每一个自举样本中都建立一个回归树，然后进行简单的算术平均。如果 $d^{\\star b}$ 是原数据 $d$ 的第 $b$ 个自举样本，我们可以将装袋法估计量定义为\n",
    "\n",
    "$$\n",
    "\\hat{r}_{\\mathrm{bagging}} = B^{-1} \\sum_{b=1}^B \\hat{r}( d^{\\star b} ).\n",
    "$$\n",
    "\n",
    "(装袋法是一种 **集成学习**)\n",
    "\n",
    "{cite}`inoue2008useful` 演示了装袋法在时间序列预测中的应用。\n",
    "{cite}`hirano2017forecasting` 提供了一个减少装袋法风险的新理论视角。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7929952",
   "metadata": {},
   "source": [
    "\n",
    "## 随机森林\n",
    "\n",
    "**随机森林** [{cite}`breiman2001random`] 在总体 $p$ 中随机抽取样本 $m$ 来构造 **子树** 进行分类。\n",
    "随机森林中的调谐参数是树的深度和 $m$ 。 由于抽样回归的“去相关性”，一般情况下它的预测效果比装袋法更好。\n",
    "\n",
    "下方是使用波士顿住房数据的随机森林的简单例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df69873",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "require(randomForest)\n",
    "require(MASS) # Package which contains the Boston housing dataset\n",
    "attach(Boston)\n",
    "set.seed(101)\n",
    "\n",
    "# training Sample with 300 observations\n",
    "train <- sample(1:nrow(Boston), 300)\n",
    "\n",
    "Boston.rf <- randomForest(medv ~ ., data = Boston, subset = train)\n",
    "plot(Boston.rf)\n",
    "\n",
    "importance(Boston.rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d493c922",
   "metadata": {},
   "source": [
    "尽管算法十分简单，随机森林的相合性直到2015年才被{cite}`scornet2015consistency`证明；而推断理论则是由{cite}`wager2018estimation`首先提出；{cite}`athey2019generalized` 将 CART 推广到局部极大似然。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0fac77",
   "metadata": {},
   "source": [
    "## 梯度提升\n",
    "\n",
    "装袋法和随机森林在聚合回归树时总是采用相同的权重。然而，**提升树** 法用一种特别的方式来决定权重。它是一种确定性方法，不对原数据进行重抽样。\n",
    "\n",
    "1. 用原数据 $d^0=(x_i,y_i)$ 生成一个较浅的回归树 $\\hat{r}^{0}(d^0)$ 。保留预测结果 $f^0_i = \\alpha \\cdot \\hat{r}^0 (d^0, x_i)$ (其中\n",
    "$\\alpha\\in [0,1]$ 是一个压缩调谐参数)和残差 $e_i^{0} = y_i - f^0_i$ ，令 $m=1$ 。\n",
    "2. 在第 $m$ 次迭代中, 使用数据 $d^m = (x_i,e_i^{m-1})$ 生成浅回归树 $\\hat{r}^{m}(d^m)$ 。保留预测结果 $f^m_i =  f^{m-1}_i +  \\alpha \\cdot \\hat{r}^m (d, x_i)$ 和残差 $e_i^{m} = y_i - f^m_i$ 。使 $m = m+1$ 。\n",
    "3.重复步骤2直到 $m > M$.\n",
    "\n",
    "在这个提升算法中有三个调谐参数：树的深度、压缩水平 $\\alpha$ 和迭代次数 $M$ 。\n",
    "算法对于三个参数都很敏感，当参数设定合理时，最终效果会非常显著。例如{cite}`lin2020`指出，脚本 `Beijing_housing_gbm.R` 得到的样本外 $R^2$ 比OLS显著高出不少。该脚本使用 `gbm` (Gradient Boosting Machine) 包进行梯度提升运算。\n",
    "\n",
    "Boosting算法不只这一种。例如 $L_2$ 提升、 逐项提升 (componentwise boosting) 和适应性提升 (adaBoost) 等等。统计学者们将Boosting视为梯度下降算法来降低风险。每次迭代中拟合的树的下降速度都是最快的，而压缩会放缓拟合的速度来避免算法过度贪婪。\n",
    "\n",
    "* {cite}`shi2016econometric` 提出了一种与Boosting思路类似的greedy算法，并用来挑选GMM中的矩条件。\n",
    "* {cite}`phillips2019boosting` 用 $L_2$ 提升作为Hodrick-Prescott滤波的进阶版。\n",
    "* {cite}`shi2019forward`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcdfa1b",
   "metadata": {},
   "source": [
    "## 神经网络\n",
    "\n",
    "神经网络是Alpha-Go和自动驾驶背后的支撑技术。但在统计学者看来，它只不过是一种特殊的非线性模型。\n",
    "图1展示了一个单层的神经网络。不过，大多数情况下神经网络都不只一层，从 $k-1$ 层到 $k$ 层的过渡可以写成\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z_l^{(k)} & =  w_{l0}^{(k-1)} + \\sum_{j=1}^{p_{k-1} } w_{lj}^{(k-1)} a_j^{(k-1)} \\\\ \n",
    "a_l^{(k)} & =  g^{(k)} ( z_l^{(k)}).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "其中 $a_j^{(0)}  = x_j$  是输入值， $z_l^{(k)}$ 是第 $k$ 个隐藏层，当中所有的 $w$ 都是要估计的系数。\n",
    "上述公式表明 $z_l^{(k)}$ 通常是线性形式，**激励函数** $g(\\cdot)$ 是恒等函数或者简单的非线性函数。\n",
    "激励函数通常选择 sigmoid 函数 ($1/(1+\\exp(-x))$) 或者线性整流函数 (ReLu, $z\\cdot 1\\{x\\geq 0\\}$)。\n",
    "\n",
    "\n",
    "![A Single Layer Feedforward Neural Network (from Wiki)](graph/Colored_neural_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5be276",
   "metadata": {},
   "source": [
    "拟合神经网络通常要考虑几个因素：除激励函数外，隐藏层的数量和每一层的节点也是重要的调谐参数。每一层和每一个节点都会生成许多的自由参数，然后可以用正则化方法对于 $l_1$ 与 $l_2$ 规范进行惩罚。\n",
    "`data_example/Keras_ANN.R` 给出一个两层隐藏层的神经网络的例子(每层有64个节点)，并使用了线性整流激励函数。\n",
    "\n",
    "由于神经网络本身是非线性，目前对其尚未有明确的理论解释。早期的一个理论贡献来自于{cite}`hornik1989multilayer`的定理2.2：给定足够多的节点，一个单层神经网络是任何可测函数的**万能近似器**。\n",
    "\n",
    "在建立一个神经网络后，自由参数必须要用数值最优化决定。非线性的复杂结构使得最优化极具挑战性，我们很难能够得到全局的最优值。尤其当样本数量较大时，最优化算法是随机梯度下降的。 \n",
    "\n",
    "归功于计算科学家们的努力， 谷歌的 `tensorflow` 是一个深受欢迎的神经网络后端， `keras` 是深度学习建模语言。\n",
    "它们之间的关系类似于 `Mosek` 和 `CVXR` 。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a055a836",
   "metadata": {},
   "source": [
    "## 随机梯度下降\n",
    "\n",
    "在最优化过程中， 我们迭代 $D$ 维参数\n",
    "\n",
    "$$\n",
    "\\beta_{k+1} = \\beta_{k} + a_k p_k,\n",
    "$$\n",
    "\n",
    "其中 $a_k \\in \\mathbb{R}$ 是步长， $p_k\\in \\mathbb{R}^D$ 是方向向量。使用泰勒级数展开可以得到：\n",
    "\n",
    "$$\n",
    "f(\\beta_{k+1}) = f(\\beta_k + a_k p_k ) \\approx f(\\beta_k) + a_k \\nabla f(\\beta_k) p_k,\n",
    "$$\n",
    "\n",
    "如果我们希望目标函数 $f(x)$ 在每一步中递减，那么需要 $\\nabla f(\\beta_k) p_k \\leq 0$ 。\n",
    "我们可以令 $p_k =-\\nabla f(\\beta_k)$ ，这种方式通常称为**最深下降**(the deepest descent)。\n",
    "牛顿法则是令 $p_k =- (\\nabla^2 f(\\beta_k))^{-1}  \\nabla f(\\beta_k)$ ，\n",
    "然后 BFGS 用一个低阶矩阵来估计 $\\nabla^2 f(\\beta_k)$ 。 线性搜索是一维问题，可以通过精确最小化或回溯来解决。下降法的具体细节可以参考  {cite}`boyd2004convex` 的 9.2--9.5 章节。\n",
    "\n",
    "当样本规模大且参数数量过多时，梯度的求解将会非常的复杂。随机梯度下降 (SGD) 在每次迭代中都只用一小部分的样本估计梯度，这样可以大大减少计算的时间。 它是神经网络训练这种复杂的最优化问题中的 **事实** 最优化过程。\n",
    "\n",
    "不过， SGD 涉及到了调谐参数 (比如batch的大小和学习速率，学习速率代替了步长 $a_k$ 并变成了正则参数)。这些参数将会极大地影响最终结果，这种情况在非线性问题中更加显著。因此，在真正实操之前，算法必须经过仔细的测试。\n",
    "\n",
    "\n",
    "下面是一个PPMLE中SGD的例子，在最优化那节课中也提到过这个例子。在这里它的样本规模是100，000且参数数量为100。通常情况下， SGD 比 `nlopt`要快很多。\n",
    "\n",
    "由于在SGD中每次的对数似然方程和梯度都是在一个不同的子样本中计算的，所以在此我们将带有数据的新函数定义为arguments。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257d5b04",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "poisson.loglik <- function(b, y, X) {\n",
    "  b <- as.matrix(b)\n",
    "  lambda <- exp(X %*% b)\n",
    "  ell <- -mean(-lambda + y * log(lambda))\n",
    "  return(ell)\n",
    "}\n",
    "\n",
    "\n",
    "poisson.loglik.grad <- function(b, y, X) {\n",
    "  b <- as.matrix(b)\n",
    "  lambda <- as.vector(exp(X %*% b))\n",
    "  ell <- -colMeans(-lambda * X + y * X)\n",
    "  ell_eta <- ell\n",
    "  return(ell_eta)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40acc76d",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "##### generate the artificial data\n",
    "set.seed(898)\n",
    "nn <- 1e5\n",
    "K <- 100\n",
    "X <- cbind(1, matrix(runif(nn * (K - 1)), ncol = K - 1))\n",
    "b0 <- rep(1, K) / K\n",
    "y <- rpois(nn, exp(X %*% b0))\n",
    "\n",
    "b.init <- runif(K)\n",
    "b.init <- 2 * b.init / sum(b.init)\n",
    "# and these tuning parameters are related to N and K\n",
    "\n",
    "n <- length(y)\n",
    "test_ind <- sample(1:n, round(0.2 * n))\n",
    "\n",
    "y_test <- y[test_ind]\n",
    "X_test <- X[test_ind, ]\n",
    "\n",
    "y_train <- y[-test_ind ]\n",
    "X_train <- X[-test_ind, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9406f10c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# optimization parameters\n",
    "# sgd depends on\n",
    "\n",
    "# * eta: the learning rate\n",
    "# * epoch: the averaging small batch\n",
    "# * the initial value\n",
    "\n",
    "max_iter <- 5000\n",
    "min_iter <- 20\n",
    "eta <- 0.01\n",
    "epoch <- round(100 * sqrt(K))\n",
    "\n",
    "b_old <- b.init\n",
    "\n",
    "pts0 <- Sys.time()\n",
    "# the iteration of gradient\n",
    "for (i in 1:max_iter) {\n",
    "  loglik_old <- poisson.loglik(b_old, y_train, X_train)\n",
    "  i_sample <- sample(1:length(y_train), epoch, replace = TRUE)\n",
    "  b_new <- b_old - eta * poisson.loglik.grad(b_old, y_train[i_sample], X_train[i_sample, ])\n",
    "  loglik_new <- poisson.loglik(b_new, y_test, X_test)\n",
    "  b_old <- b_new # update\n",
    "\n",
    "  criterion <- loglik_old - loglik_new\n",
    "\n",
    "  if (criterion < 0.0001 & i >= min_iter) break\n",
    "}\n",
    "cat(\"point estimate =\", b_new, \", log_lik = \", loglik_new, \"\\n\")\n",
    "pts1 <- Sys.time() - pts0\n",
    "print(pts1)\n",
    "\n",
    "\n",
    "# optimx is too slow for this dataset.\n",
    "# Nelder-Mead method is too slow for this dataset\n",
    "\n",
    "# thus we only sgd with NLoptr\n",
    "\n",
    "opts <- list(\"algorithm\" = \"NLOPT_LD_SLSQP\", \"xtol_rel\" = 1.0e-7, maxeval = 5000)\n",
    "\n",
    "\n",
    "pts0 <- Sys.time()\n",
    "res_BFGS <- nloptr::nloptr(\n",
    "  x0 = b.init,\n",
    "  eval_f = poisson.loglik,\n",
    "  eval_grad_f = poisson.loglik.grad,\n",
    "  opts = opts,\n",
    "  y = y_train, X = X_train\n",
    ")\n",
    "pts1 <- Sys.time() - pts0\n",
    "print(pts1)\n",
    "\n",
    "b_hat_nlopt <- res_BFGS$solution\n",
    "\n",
    "#### evaluation in the test sample\n",
    "cat(\"log lik in test data by sgd = \", poisson.loglik(b_new, y = y_test, X_test), \"\\n\")\n",
    "cat(\"log lik in test data by nlopt = \", poisson.loglik(b_hat_nlopt, y = y_test, X_test), \"\\n\")\n",
    "cat(\"log lik in test data by true para. = \", poisson.loglik(b0, y = y_test, X_test), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ea6bf5",
   "metadata": {},
   "source": [
    "## 拓展阅读\n",
    "\n",
    "* Efron and Hastie: 第 8、 17 和 18章节\n",
    "\n",
    "## 引言\n",
    "\n",
    "<!-- \"The world is yours, as well as ours, but in the last analysis, it is yours. You young people, full of vigor and vitality, are in the bloom of life, like the sun at eight or nine in the morning. Our hope is placed on you.\" -->\n",
    "\n",
    "<!-- ---Mao Zedong, Talk at a meeting with Chinese students and trainees in Moscow (November 17, 1957). -->\n",
    "\n",
    "## 参考文献"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "R",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
