
<!DOCTYPE html>

<html lang="zh_CN">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Prediction-Oriented Algorithms &#8212; 经济学中的“数据科学”</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="索引" href="genindex.html" />
    <link rel="search" title="搜索" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="zh_CN">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">经济学中的“数据科学”</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="00-preface.html">
                    Preface
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01-basic_R-CN.html">
   1. 基础R语言
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03-advanced_R-CN.html">
   3. R语言进阶
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05-integration-CN.html">
   4. 积分
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06-simulation-CN.html">
   5. 模拟
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07-optimization-CN.html">
   6. 数值最优化
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08-ML-CN.html">
   7. 从非参数到机器学习
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09-ML2-CN.html">
   8. 基于预测的算法
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/zhentaoshi/econ_data_science/master?urlpath=tree/docs/09-ML2.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/zhentaoshi/econ_data_science"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/zhentaoshi/econ_data_science/issues/new?title=Issue%20on%20page%20%2F09-ML2.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/09-ML2.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regression-tree-and-bagging">
   Regression Tree and Bagging
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#random-forest">
   Random Forest
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-boosting">
   Gradient Boosting
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#neural-network">
   Neural Network
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stochastic-gradient-descent">
   Stochastic Gradient Descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reading">
   Reading
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quotation">
   Quotation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Prediction-Oriented Algorithms</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regression-tree-and-bagging">
   Regression Tree and Bagging
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#random-forest">
   Random Forest
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-boosting">
   Gradient Boosting
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#neural-network">
   Neural Network
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stochastic-gradient-descent">
   Stochastic Gradient Descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reading">
   Reading
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quotation">
   Quotation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="prediction-oriented-algorithms">
<h1>Prediction-Oriented Algorithms<a class="headerlink" href="#prediction-oriented-algorithms" title="永久链接至标题">#</a></h1>
<p>In this lecture, we introduce supervised learning methods that induces
data-driven interaction of the covariates.
The interaction makes the covariates much more flexible
to capture the subtle feature in the data.
However, insufficient theoretical understanding is shed little light on these methods due to
the complex nature, so they are often viewed by theorists as “black-boxes” methods.
In real applications, when the machines are carefully tuned, they
can achieve surprisingly superior performance.
&#64;gu2018empirical showcase a horse race of a myriad of methods,
and the general message is that interaction helps with forecast in
the financial market.
In the meantime, industry insiders are pondering whether these methods are “alchemy”
which fall short of scientific standard. Caution must be exercised when
we apply these methods in empirical economic analysis.</p>
<section id="regression-tree-and-bagging">
<h2>Regression Tree and Bagging<a class="headerlink" href="#regression-tree-and-bagging" title="永久链接至标题">#</a></h2>
<p>We consider supervised learning setting in which we use <span class="math notranslate nohighlight">\(x\)</span> to predict <span class="math notranslate nohighlight">\(y\)</span>.
It can be done by traditional nonparametric methods such as kernel regression.
<em>Regression tree</em> [&#64;breiman1984classification] is an alternative to kernel regression. Regression tree  recursively partitions the
the space of the regressors. The algorithm is easy to describe: each time a covariate is split into two dummies, and the splitting criterion is aggressive reduction of the SSR. In the formulate of the SSR, the fitted value is computed as the average of <span class="math notranslate nohighlight">\(y_i\)</span>’s in a partition.</p>
<p>The tuning parameter is the depth of the tree, which is referred to the number of splits.
Given a dataset <span class="math notranslate nohighlight">\(d\)</span> and the depth of the tree, the fitted regression tree <span class="math notranslate nohighlight">\(\hat{r}(d)\)</span> is completely determined by the data.</p>
<p>The problem of the regression tree is its instability. For data generated
from the same data generating process (DGP), the  covariate chosen to be split and the splitting point can
vary widely and they heavily influence the path of the partition.</p>
<p><em>Bootstrap averaging</em>, or <em>bagging</em> for short, was introduced to reduce the
variance of the regression tree [&#64;breiman1996bagging]. Bagging grows a regression tree for
each bootstrap sample, and then do a simple average. Let <span class="math notranslate nohighlight">\(d^{\star b}\)</span>
be the <span class="math notranslate nohighlight">\(b\)</span>-th bootstrap sample of the original data <span class="math notranslate nohighlight">\(d\)</span>, and then the
bagging estimator is defined as</p>
<div class="math notranslate nohighlight">
\[
\hat{r}_{\mathrm{bagging}} = B^{-1} \sum_{b=1}^B \hat{r}( d^{\star b} ).
\]</div>
<p>Bagging is an example of the <em>ensemble learning</em>.</p>
<p>&#64;inoue2008useful is an early application of bagging in time series forecast.
&#64;hirano2017forecasting provide a theoretical perspective on the risk reduction of bagging.</p>
</section>
<section id="random-forest">
<h2>Random Forest<a class="headerlink" href="#random-forest" title="永久链接至标题">#</a></h2>
<p><em>Random forest</em> [&#64;breiman2001random] shakes up the regressors by randomly sampling <span class="math notranslate nohighlight">\(m\)</span> out of the total <span class="math notranslate nohighlight">\(p\)</span> covarites before <em>each split of a tree</em>.
The tuning parameters in random forest
is the tree depth and <span class="math notranslate nohighlight">\(m\)</span>. Due to the “de-correlation” in sampling the regressors, it in
general performs better than bagging in prediction exercises.</p>
<p>Below is a very simple real data example of random forest using the
Boston Housing data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">require</span><span class="p">(</span><span class="n">randomForest</span><span class="p">)</span>
<span class="nf">require</span><span class="p">(</span><span class="n">MASS</span><span class="p">)</span> <span class="c1"># Package which contains the Boston housing dataset</span>
<span class="nf">attach</span><span class="p">(</span><span class="n">Boston</span><span class="p">)</span>
<span class="nf">set.seed</span><span class="p">(</span><span class="m">101</span><span class="p">)</span>

<span class="c1"># training Sample with 300 observations</span>
<span class="n">train</span> <span class="o">&lt;-</span> <span class="nf">sample</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="nf">nrow</span><span class="p">(</span><span class="n">Boston</span><span class="p">),</span> <span class="m">300</span><span class="p">)</span>

<span class="n">Boston.rf</span> <span class="o">&lt;-</span> <span class="nf">randomForest</span><span class="p">(</span><span class="n">medv</span> <span class="o">~</span> <span class="n">.</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">Boston</span><span class="p">,</span> <span class="n">subset</span> <span class="o">=</span> <span class="n">train</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">Boston.rf</span><span class="p">)</span>

<span class="nf">importance</span><span class="p">(</span><span class="n">Boston.rf</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Despite the simplicity of the algorithm, the consistency of random forest is not proved
until &#64;scornet2015consistency, and
inferential theory was first established by
&#64;wager2018estimation  in the context of treatment effect estimation.
&#64;athey2019generalized generalizes CART to local maximum likelihood.</p>
<p><strong>Example</strong>: Random forest for Survey of Professional Forecasters in <code class="docutils literal notranslate"><span class="pre">data_example/SPF_RF.R</span></code> from &#64;cheng2020survey. The script uses <code class="docutils literal notranslate"><span class="pre">caret</span></code> framework.</p>
</section>
<section id="gradient-boosting">
<h2>Gradient Boosting<a class="headerlink" href="#gradient-boosting" title="永久链接至标题">#</a></h2>
<p>Bagging and random forest almost always use equal weight on each generated tree
for the ensemble.
Instead, <em>tree boosting</em> takes a distinctive scheme to determine the ensemble weights.
It is a deterministic approach that does not resample the original data.</p>
<ol class="simple">
<li><p>Use the original data <span class="math notranslate nohighlight">\(d^0=(x_i,y_i)\)</span> to grow a shallow tree <span class="math notranslate nohighlight">\(\hat{r}^{0}(d^0)\)</span>. Save the prediction <span class="math notranslate nohighlight">\(f^0_i = \alpha \cdot \hat{r}^0 (d^0, x_i)\)</span> where
<span class="math notranslate nohighlight">\(\alpha\in [0,1]\)</span> is a shrinkage tuning parameter. Save
the residual <span class="math notranslate nohighlight">\(e_i^{0} = y_i - f^0_i\)</span>. Set <span class="math notranslate nohighlight">\(m=1\)</span>.</p></li>
<li><p>In the <span class="math notranslate nohighlight">\(m\)</span>-th iteration, use the data <span class="math notranslate nohighlight">\(d^m = (x_i,e_i^{m-1})\)</span> to grow a shallow tree <span class="math notranslate nohighlight">\(\hat{r}^{m}(d^m)\)</span>. Save the prediction <span class="math notranslate nohighlight">\(f^m_i =  f^{m-1}_i +  \alpha \cdot \hat{r}^m (d, x_i)\)</span>. Save
the residual <span class="math notranslate nohighlight">\(e_i^{m} = y_i - f^m_i\)</span>. Update <span class="math notranslate nohighlight">\(m = m+1\)</span>.</p></li>
<li><p>Repeat Step 2 until <span class="math notranslate nohighlight">\(m &gt; M\)</span>.</p></li>
</ol>
<p>In this boosting algorithm there are three tuning parameters: the tree depth,  the shrinkage level <span class="math notranslate nohighlight">\(\alpha\)</span>, and the number of iterations <span class="math notranslate nohighlight">\(M\)</span>.
The algorithm can be sensitive to all the three tuning parameters.
When a model is tuned well, it often performs remarkably.
For example, the script <code class="docutils literal notranslate"><span class="pre">Beijing_housing_gbm.R</span></code> achieves much higher out-of-sample <span class="math notranslate nohighlight">\(R^2\)</span> than OLS, reported in &#64;lin2020. This script implements boosting via the package <code class="docutils literal notranslate"><span class="pre">gbm</span></code>, which stands for “Gradient Boosting Machine.”</p>
<p>There are many variants of boosting algorithms. For example, <span class="math notranslate nohighlight">\(L_2\)</span>-boosting, componentwise boosting, and AdaBoosting, etc. Statisticians view boosting as a gradient descent algorithm to reduce the risk. The fitted
tree in each iteration is the deepest descent direction, while the shrinkage tames the fitting to avoid proceeding too aggressively.</p>
<ul class="simple">
<li><p>&#64;shi2016econometric proposes a greedy algorithm in similar spirit to boosting for moment selection in GMM.</p></li>
<li><p>&#64;phillips2019boosting uses <span class="math notranslate nohighlight">\(L_2\)</span>-boosting as a boosted version of the Hodrick-Prescott filter.</p></li>
<li><p>&#64;shi2019forward</p></li>
</ul>
</section>
<section id="neural-network">
<h2>Neural Network<a class="headerlink" href="#neural-network" title="永久链接至标题">#</a></h2>
<p>A neural network is the workhorse behind Alpha-Go and self-driven cars.
However, from a statistician’s point of view it is just a particular type of nonlinear models.
Figure 1 illustrates a one-layer neural network, but in general there can be several layers.
The transition from layer <span class="math notranslate nohighlight">\(k-1\)</span> to layer <span class="math notranslate nohighlight">\(k\)</span> can be written as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
z_l^{(k)} &amp; =  w_{l0}^{(k-1)} + \sum_{j=1}^{p_{k-1} } w_{lj}^{(k-1)} a_j^{(k-1)} \\ 
a_l^{(k)} &amp; =  g^{(k)} ( z_l^{(k)}),
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(a_j^{(0)}  = x_j\)</span>  is the input,  <span class="math notranslate nohighlight">\(z_l^{(k)}\)</span> is the <span class="math notranslate nohighlight">\(k\)</span>-th hidden layer,
and all the <span class="math notranslate nohighlight">\(w\)</span>s are coefficients to be estimated.
The above formulation shows that  <span class="math notranslate nohighlight">\(z_l^{(k)}\)</span> usually takes a linear form,
while the <em>activation function</em> <span class="math notranslate nohighlight">\(g(\cdot)\)</span> can be an identity function or a simple nonlinear function.
Popular choices of the activation function are sigmoid (<span class="math notranslate nohighlight">\(1/(1+\exp(-x))\)</span>) and rectified linear unit (ReLu, <span class="math notranslate nohighlight">\(z\cdot 1\{x\geq 0\}\)</span>), etc.</p>
<p><img alt="A Single Layer Feedforward Neural Network (from Wiki)" src="_images/Colored_neural_network.png" /></p>
<p>A user has several decisions to make when fitting a neural network:
besides the activation function,
the tuning parameters are the number of hidden layers and the number of nodes in each layer.
Many free parameters are generated from the multiple layer and multiple nodes,
and in estimation often regularization methods are employed to penalize
the <span class="math notranslate nohighlight">\(l_1\)</span> and/or <span class="math notranslate nohighlight">\(l_2\)</span> norms, which requires extra tuning parameters.
<code class="docutils literal notranslate"><span class="pre">data_example/Keras_ANN.R</span></code> gives an example of a neural network
with two hidden layers, each has 64 nodes, and the ReLu activation function.</p>
<p>Due to the nonlinear nature of the neural networks, theoretical understanding about its behavior is still scant. One of the early contributions came from econometrician: &#64;hornik1989multilayer
(Theorem 2.2) show that a single hidden layer neural network, given enough many nodes, is a <em>universal approximator</em> for any
measurable function.</p>
<p>After setting up a neural network, the free parameters must be determined by
numerical optimization. The nonlinear complex structure makes the optimization
very challenging and the global optimizer is beyond guarantee.
In particular, when the sample size is big, the de facto optimization algorithm
is the stochastic gradient descent.</p>
<p>Thanks to computational
scientists, Google’s <code class="docutils literal notranslate"><span class="pre">tensorflow</span></code> is a popular backend of
neural network estimation, and <code class="docutils literal notranslate"><span class="pre">keras</span></code> is the deep learning modeling language.
Their relationship is similar to <code class="docutils literal notranslate"><span class="pre">Mosek</span></code> and <code class="docutils literal notranslate"><span class="pre">CVXR</span></code>.</p>
</section>
<section id="stochastic-gradient-descent">
<h2>Stochastic Gradient Descent<a class="headerlink" href="#stochastic-gradient-descent" title="永久链接至标题">#</a></h2>
<p>In optimization we update the <span class="math notranslate nohighlight">\(D\)</span>-dimensional parameter</p>
<div class="math notranslate nohighlight">
\[
\beta_{k+1} = \beta_{k} + a_k p_k,
\]</div>
<p>where <span class="math notranslate nohighlight">\(a_k \in \mathbb{R}\)</span> is the step length and <span class="math notranslate nohighlight">\(p_k\in \mathbb{R}^D\)</span> is a vector
of directions. Use a Talyor expansion,</p>
<div class="math notranslate nohighlight">
\[
f(\beta_{k+1}) = f(\beta_k + a_k p_k ) \approx f(\beta_k) + a_k \nabla f(\beta_k) p_k,
\]</div>
<p>If in each step we want the value of the criterion function
<span class="math notranslate nohighlight">\(f(x)\)</span> to decrease, we need <span class="math notranslate nohighlight">\(\nabla f(\beta_k) p_k \leq 0\)</span>.
A simple choice is <span class="math notranslate nohighlight">\(p_k =-\nabla f(\beta_k)\)</span>, which is called the deepest decent.
Newton’s method corresponds to <span class="math notranslate nohighlight">\(p_k =- (\nabla^2 f(\beta_k))^{-1}  \nabla f(\beta_k)\)</span>,
and BFGS uses a low-rank matrix to approximate <span class="math notranslate nohighlight">\(\nabla^2 f(\beta_k)\)</span>. The linear search is a one-dimensional problem and it can be handled by either exact minimization or backtracking. Details of the descent method is referred to Chapter 9.2–9.5 of &#64;boyd2004convex.</p>
<p>When the sample size is huge and the number of parameters is also big,
the evaluation of the gradient can be prohibitively expensive.
Stochastic gradient descent (SGD) uses a small batch of the sample
to evaluate the gradient in each iteration. It can significantly save
computational time. It is the <em>de facto</em> optimization procedure in complex optimization problems such as
training a neural network.</p>
<p>However, SGD involves tuning parameters (say, the batch size and the learning rate. Learning rate replaces the step length <span class="math notranslate nohighlight">\(a_k\)</span> and becomes a regularization parameter.)
that can dramatically affect
the outcome, in particular in nonlinear problems.
Careful experiments must be carried out before serious implementation.</p>
<p>Below is an example of SGD in the PPMLE that we visited in the lecture of optimization, now with sample size 100,000 and
the number of parameters 100. SGD is usually much faster than <code class="docutils literal notranslate"><span class="pre">nlopt</span></code>.</p>
<p>The new functions are defined with the data explicitly as arguments.
Because in SGD each time the log-likelihood function and the gradient are
evaluated at a different subsample.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">poisson.loglik</span> <span class="o">&lt;-</span> <span class="nf">function</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">b</span> <span class="o">&lt;-</span> <span class="nf">as.matrix</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
  <span class="n">lambda</span> <span class="o">&lt;-</span> <span class="nf">exp</span><span class="p">(</span><span class="n">X</span> <span class="o">%*%</span> <span class="n">b</span><span class="p">)</span>
  <span class="n">ell</span> <span class="o">&lt;-</span> <span class="o">-</span><span class="nf">mean</span><span class="p">(</span><span class="o">-</span><span class="n">lambda</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="nf">log</span><span class="p">(</span><span class="n">lambda</span><span class="p">))</span>
  <span class="nf">return</span><span class="p">(</span><span class="n">ell</span><span class="p">)</span>
<span class="p">}</span>


<span class="n">poisson.loglik.grad</span> <span class="o">&lt;-</span> <span class="nf">function</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">b</span> <span class="o">&lt;-</span> <span class="nf">as.matrix</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
  <span class="n">lambda</span> <span class="o">&lt;-</span> <span class="nf">as.vector</span><span class="p">(</span><span class="nf">exp</span><span class="p">(</span><span class="n">X</span> <span class="o">%*%</span> <span class="n">b</span><span class="p">))</span>
  <span class="n">ell</span> <span class="o">&lt;-</span> <span class="o">-</span><span class="nf">colMeans</span><span class="p">(</span><span class="o">-</span><span class="n">lambda</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">X</span><span class="p">)</span>
  <span class="n">ell_eta</span> <span class="o">&lt;-</span> <span class="n">ell</span>
  <span class="nf">return</span><span class="p">(</span><span class="n">ell_eta</span><span class="p">)</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">##### generate the artificial data</span>
<span class="nf">set.seed</span><span class="p">(</span><span class="m">898</span><span class="p">)</span>
<span class="n">nn</span> <span class="o">&lt;-</span> <span class="m">1e5</span>
<span class="n">K</span> <span class="o">&lt;-</span> <span class="m">100</span>
<span class="n">X</span> <span class="o">&lt;-</span> <span class="nf">cbind</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="nf">matrix</span><span class="p">(</span><span class="nf">runif</span><span class="p">(</span><span class="n">nn</span> <span class="o">*</span> <span class="p">(</span><span class="n">K</span> <span class="o">-</span> <span class="m">1</span><span class="p">)),</span> <span class="n">ncol</span> <span class="o">=</span> <span class="n">K</span> <span class="o">-</span> <span class="m">1</span><span class="p">))</span>
<span class="n">b0</span> <span class="o">&lt;-</span> <span class="nf">rep</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span> <span class="o">/</span> <span class="n">K</span>
<span class="n">y</span> <span class="o">&lt;-</span> <span class="nf">rpois</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="nf">exp</span><span class="p">(</span><span class="n">X</span> <span class="o">%*%</span> <span class="n">b0</span><span class="p">))</span>

<span class="n">b.init</span> <span class="o">&lt;-</span> <span class="nf">runif</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
<span class="n">b.init</span> <span class="o">&lt;-</span> <span class="m">2</span> <span class="o">*</span> <span class="n">b.init</span> <span class="o">/</span> <span class="nf">sum</span><span class="p">(</span><span class="n">b.init</span><span class="p">)</span>
<span class="c1"># and these tuning parameters are related to N and K</span>

<span class="n">n</span> <span class="o">&lt;-</span> <span class="nf">length</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">test_ind</span> <span class="o">&lt;-</span> <span class="nf">sample</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="n">n</span><span class="p">,</span> <span class="nf">round</span><span class="p">(</span><span class="m">0.2</span> <span class="o">*</span> <span class="n">n</span><span class="p">))</span>

<span class="n">y_test</span> <span class="o">&lt;-</span> <span class="n">y</span><span class="p">[</span><span class="n">test_ind</span><span class="p">]</span>
<span class="n">X_test</span> <span class="o">&lt;-</span> <span class="n">X</span><span class="p">[</span><span class="n">test_ind</span><span class="p">,</span> <span class="p">]</span>

<span class="n">y_train</span> <span class="o">&lt;-</span> <span class="n">y</span><span class="p">[</span><span class="o">-</span><span class="n">test_ind</span> <span class="p">]</span>
<span class="n">X_train</span> <span class="o">&lt;-</span> <span class="n">X</span><span class="p">[</span><span class="o">-</span><span class="n">test_ind</span><span class="p">,</span> <span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># optimization parameters</span>
<span class="c1"># sgd depends on</span>

<span class="c1"># * eta: the learning rate</span>
<span class="c1"># * epoch: the averaging small batch</span>
<span class="c1"># * the initial value</span>

<span class="n">max_iter</span> <span class="o">&lt;-</span> <span class="m">5000</span>
<span class="n">min_iter</span> <span class="o">&lt;-</span> <span class="m">20</span>
<span class="n">eta</span> <span class="o">&lt;-</span> <span class="m">0.01</span>
<span class="n">epoch</span> <span class="o">&lt;-</span> <span class="nf">round</span><span class="p">(</span><span class="m">100</span> <span class="o">*</span> <span class="nf">sqrt</span><span class="p">(</span><span class="n">K</span><span class="p">))</span>

<span class="n">b_old</span> <span class="o">&lt;-</span> <span class="n">b.init</span>

<span class="n">pts0</span> <span class="o">&lt;-</span> <span class="nf">Sys.time</span><span class="p">()</span>
<span class="c1"># the iteration of gradient</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span> <span class="n">in</span> <span class="m">1</span><span class="o">:</span><span class="n">max_iter</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">loglik_old</span> <span class="o">&lt;-</span> <span class="nf">poisson.loglik</span><span class="p">(</span><span class="n">b_old</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_train</span><span class="p">)</span>
  <span class="n">i_sample</span> <span class="o">&lt;-</span> <span class="nf">sample</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="nf">length</span><span class="p">(</span><span class="n">y_train</span><span class="p">),</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">replace</span> <span class="o">=</span> <span class="kc">TRUE</span><span class="p">)</span>
  <span class="n">b_new</span> <span class="o">&lt;-</span> <span class="n">b_old</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="nf">poisson.loglik.grad</span><span class="p">(</span><span class="n">b_old</span><span class="p">,</span> <span class="n">y_train</span><span class="p">[</span><span class="n">i_sample</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[</span><span class="n">i_sample</span><span class="p">,</span> <span class="p">])</span>
  <span class="n">loglik_new</span> <span class="o">&lt;-</span> <span class="nf">poisson.loglik</span><span class="p">(</span><span class="n">b_new</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">X_test</span><span class="p">)</span>
  <span class="n">b_old</span> <span class="o">&lt;-</span> <span class="n">b_new</span> <span class="c1"># update</span>

  <span class="n">criterion</span> <span class="o">&lt;-</span> <span class="n">loglik_old</span> <span class="o">-</span> <span class="n">loglik_new</span>

  <span class="nf">if </span><span class="p">(</span><span class="n">criterion</span> <span class="o">&lt;</span> <span class="m">0.0001</span> <span class="o">&amp;</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="n">min_iter</span><span class="p">)</span> <span class="n">break</span>
<span class="p">}</span>
<span class="nf">cat</span><span class="p">(</span><span class="s">&quot;point estimate =&quot;</span><span class="p">,</span> <span class="n">b_new</span><span class="p">,</span> <span class="s">&quot;, log_lik = &quot;</span><span class="p">,</span> <span class="n">loglik_new</span><span class="p">,</span> <span class="s">&quot;\n&quot;</span><span class="p">)</span>
<span class="n">pts1</span> <span class="o">&lt;-</span> <span class="nf">Sys.time</span><span class="p">()</span> <span class="o">-</span> <span class="n">pts0</span>
<span class="nf">print</span><span class="p">(</span><span class="n">pts1</span><span class="p">)</span>


<span class="c1"># optimx is too slow for this dataset.</span>
<span class="c1"># Nelder-Mead method is too slow for this dataset</span>

<span class="c1"># thus we only sgd with NLoptr</span>

<span class="n">opts</span> <span class="o">&lt;-</span> <span class="nf">list</span><span class="p">(</span><span class="s">&quot;algorithm&quot;</span> <span class="o">=</span> <span class="s">&quot;NLOPT_LD_SLSQP&quot;</span><span class="p">,</span> <span class="s">&quot;xtol_rel&quot;</span> <span class="o">=</span> <span class="m">1.0e-7</span><span class="p">,</span> <span class="n">maxeval</span> <span class="o">=</span> <span class="m">5000</span><span class="p">)</span>


<span class="n">pts0</span> <span class="o">&lt;-</span> <span class="nf">Sys.time</span><span class="p">()</span>
<span class="n">res_BFGS</span> <span class="o">&lt;-</span> <span class="n">nloptr</span><span class="o">::</span><span class="nf">nloptr</span><span class="p">(</span>
  <span class="n">x0</span> <span class="o">=</span> <span class="n">b.init</span><span class="p">,</span>
  <span class="n">eval_f</span> <span class="o">=</span> <span class="n">poisson.loglik</span><span class="p">,</span>
  <span class="n">eval_grad_f</span> <span class="o">=</span> <span class="n">poisson.loglik.grad</span><span class="p">,</span>
  <span class="n">opts</span> <span class="o">=</span> <span class="n">opts</span><span class="p">,</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span>
<span class="p">)</span>
<span class="n">pts1</span> <span class="o">&lt;-</span> <span class="nf">Sys.time</span><span class="p">()</span> <span class="o">-</span> <span class="n">pts0</span>
<span class="nf">print</span><span class="p">(</span><span class="n">pts1</span><span class="p">)</span>

<span class="n">b_hat_nlopt</span> <span class="o">&lt;-</span> <span class="n">res_BFGS</span><span class="o">$</span><span class="n">solution</span>

<span class="c1">#### evaluation in the test sample</span>
<span class="nf">cat</span><span class="p">(</span><span class="s">&quot;log lik in test data by sgd = &quot;</span><span class="p">,</span> <span class="nf">poisson.loglik</span><span class="p">(</span><span class="n">b_new</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">X_test</span><span class="p">),</span> <span class="s">&quot;\n&quot;</span><span class="p">)</span>
<span class="nf">cat</span><span class="p">(</span><span class="s">&quot;log lik in test data by nlopt = &quot;</span><span class="p">,</span> <span class="nf">poisson.loglik</span><span class="p">(</span><span class="n">b_hat_nlopt</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">X_test</span><span class="p">),</span> <span class="s">&quot;\n&quot;</span><span class="p">)</span>
<span class="nf">cat</span><span class="p">(</span><span class="s">&quot;log lik in test data by true para. = &quot;</span><span class="p">,</span> <span class="nf">poisson.loglik</span><span class="p">(</span><span class="n">b0</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">X_test</span><span class="p">),</span> <span class="s">&quot;\n&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="reading">
<h2>Reading<a class="headerlink" href="#reading" title="永久链接至标题">#</a></h2>
<ul class="simple">
<li><p>Efron and Hastie: Chapter 8, 17 and 18.</p></li>
</ul>
</section>
<section id="quotation">
<h2>Quotation<a class="headerlink" href="#quotation" title="永久链接至标题">#</a></h2>
<!-- "The world is yours, as well as ours, but in the last analysis, it is yours. You young people, full of vigor and vitality, are in the bloom of life, like the sun at eight or nine in the morning. Our hope is placed on you." -->
<!-- ---Mao Zedong, Talk at a meeting with Chinese students and trainees in Moscow (November 17, 1957). -->
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="永久链接至标题">#</a></h2>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            kernelName: "ir",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By 史震涛 Shi Zhentao<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>