
<!DOCTYPE html>

<html lang="zh_CN">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>From Nonparametrics to Machine Learning &#8212; 经济学中的“数据科学”</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="索引" href="genindex.html" />
    <link rel="search" title="搜索" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="zh_CN">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">经济学中的“数据科学”</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="00-preface.html">
                    Preface
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01-basic_R-CN.html">
   1. 基础R语言
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03-advanced_R-CN.html">
   3. R语言进阶
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05-integration-CN.html">
   4. 积分
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06-simulation-CN.html">
   5. 模拟
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07-optimization-CN.html">
   6. 数值最优化
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08-ML-CN.html">
   7. 从非参数到机器学习
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09-ML2-CN.html">
   8. 基于预测的算法
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/zhentaoshi/econ_data_science/master?urlpath=tree/docs/08-ML.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/zhentaoshi/econ_data_science"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/zhentaoshi/econ_data_science/issues/new?title=Issue%20on%20page%20%2F08-ML.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/08-ML.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#nonparametric-estimation">
   Nonparametric Estimation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-splitting">
   Data Splitting
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-validation">
     Cross Validation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variable-selection-and-prediction">
   Variable Selection and Prediction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#shrinkage-estimation-in-econometrics">
   Shrinkage Estimation in Econometrics
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#empirical-applications">
   Empirical Applications
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reading">
   Reading
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#appendix">
   Appendix
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>From Nonparametrics to Machine Learning</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#nonparametric-estimation">
   Nonparametric Estimation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-splitting">
   Data Splitting
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-validation">
     Cross Validation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variable-selection-and-prediction">
   Variable Selection and Prediction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#shrinkage-estimation-in-econometrics">
   Shrinkage Estimation in Econometrics
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#empirical-applications">
   Empirical Applications
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reading">
   Reading
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#appendix">
   Appendix
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="from-nonparametrics-to-machine-learning">
<h1>From Nonparametrics to Machine Learning<a class="headerlink" href="#from-nonparametrics-to-machine-learning" title="永久链接至标题">#</a></h1>
<p>Machine learning has quickly grown into a big field, with applications from
scientific research to daily life.
An authoritative reference is &#64;friedman2001elements,
written at the entry-year postgraduate level.
The ideas in machine learning are general and applicable to economic investigation.
&#64;athey2018impact discusses the impact of machine learning techniques
to economic analysis.
&#64;mullainathan2017machine survey a few new commonly used methods and
demonstrate them in a real example.
&#64;taddy2018technological introduces new technology <em>artificial intelligence</em>
and the implication of the underlying economic modeling.</p>
<p>The two broad classes of machine learning methods are <em>supervised learning</em> and <em>unsupervised learning</em>.
Roughly speaking, the former is about the connection between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, while the latter
is only about <span class="math notranslate nohighlight">\(X\)</span>. Instances of the former are various regression
and classification methods; those of the latter are density estimation,
principal component analysis, and clustering.
These examples are all familiar econometric problems.</p>
<p>From an econometrician’s view, supervised machine learning is a set of data fitting procedures that focus on out-of-sample prediction.
The simplest illustration is in the regression context.
We repeat a scientific experiment for <span class="math notranslate nohighlight">\(n\)</span> times, and we harvest a dataset <span class="math notranslate nohighlight">\((y_i, x_i)_{i=1}^n\)</span>.
What would be the best way to predict <span class="math notranslate nohighlight">\(y_{n+1}\)</span> from the same experiment if we know <span class="math notranslate nohighlight">\(x_{n+1}\)</span>?</p>
<p>Machine learning is a paradigm shift against conventional statistics.
When a statistician propose a new estimator, the standard practice is to pursue
three desirable properties one after another.
We first establish its consistency, which is seen as the bottom line.
Given consistency, we want to show its asymptotic distribution. Ideally, the asymptotic
distribution is normal. Asymptotic normality is desirable as it holds for many regular estimators
and the inferential procedure is familiar to applied researchers.
Furthermore, for an asymptotically normal estimator, we want to
show efficiency, an optimality property. An efficient estimator achieves the smallest asymptotic variance
in a class of asymptotically normal estimators.</p>
<p>In addition, econometrician also cares about
model identification and economic interpretation of the empirical results.
Econometrics workflow interacts the data at hand and the model of interest. At the population level,
we think about the problem of identification. Once the parameter of interest is identified,
then we can proceed to parameter estimation and inference.
Finally, we interpret the results and hopefully they shed light on economics.</p>
<!-- \begin{figure} -->
<!-- \centering -->
<!-- \begin{tikzpicture}[node distance=5mm] -->
<!--     %\draw[help lines] (0,0) grid (9,7); -->
<!-- % Nodes -->
<!--     \node [draw, ellipse] (a) at (2,6) {Model}; -->
<!--     \node [draw, ellipse] (b) at (8,6) {abstract data}; -->
<!--     \node [color=black!40](c) at (1,5) {Population}; -->
<!--     \node (d) at (5,4) {Estimation}; -->
<!--     \node [color=black!40](e) at (1,2) {Sample}; -->
<!--     \node (f) at (5,2) {Inference}; -->
<!--     \node (g) at (5,0) {Interpretation}; -->
<!--     \node [color=black!40](h) at (1,0) {Economics}; -->
<!-- % Arrows   -->
<!--     \draw[-latex] (a) to[bend right=10] node[above, yshift=2mm] {Identification}  (b); -->
<!--     \draw[-latex] (b) to[bend right=10]  (a); -->
<!--     \draw[-latex] (a) to[bend left=0]  (d); -->
<!--     \draw[-latex] (b) to[bend right=0]  (d); -->
<!--     \draw[-latex] (d) to[bend right=0]  (f); -->
<!--     \draw[-latex] (f) to[bend right=0]  (g); -->
<!-- % Lines -->
<!--     \draw[color=black!40,very thick] (0,1) -- (2,1); -->
<!--     \draw[color=black!40,very thick] (0,4) -- (2,4); -->
<!-- \end{tikzpicture} -->
<!-- \caption{Econometrics workflow} -->
<!-- \end{figure} -->
<p>Machine learning deviates from such routines.
First, they argue efficiency is not crucial because the dataset itself is big enough so that the variance is usually small.
Second, in many situations statistical inference is not the goal, so
inferential procedure is not of interest. For example, the recommendation system on Amazon or Taobao has
a machine learning algorithm behind it. There we care about the prediction accuracy, not the causal link
why a consumer interested in one good is likely to purchase another good. Third, the world is so complex
that we have little idea about how the data is generated. We do not have to assume a data generating process (DGP).
If there is no DGP, we lose the standing ground to talk about consistency. Where would my estimator converge
to if there is no “true parameter”? With these arguments, the paradigm of conventional statistics is
smashed. In the context of econometrics,
such argument completely rejects the structural modeling tradition (the Cowles approach).</p>
<p>Readers interested in the debate are referred to &#64;breiman2001statistical.
In this lecture, we put aside the ongoing philosophical debate. Instead,  we study
the most popular machine learning methods that have found growing popularity in economics.</p>
<section id="nonparametric-estimation">
<h2>Nonparametric Estimation<a class="headerlink" href="#nonparametric-estimation" title="永久链接至标题">#</a></h2>
<p><em>Parametric</em> is referred to problems with a finite number of parameters,
whereas <em>nonparametric</em> is associated with an infinite number of parameters.
Nonparametric estimation is nothing new to statisticians. However, some ideas
in this old topic is directly related to the underlying principles of machine learning methods.</p>
<p>Consider the density estimation given a sample <span class="math notranslate nohighlight">\((x_1,\ldots,x_n)\)</span>. If we assume that
the sample is drawn from a parametric family, for example the normal distribution, then we can use the
maximum likelihood estimation to learn the mean and the variance.
Nevertheless, when the parametric family is misspecified, the MLE estimation
is inconsistent in theory, and we can at best identify a <em>pseudo true value</em>.
In practice, what is the correct parametric family is unknown.
If we do not want to impose a parametric assumption, then in principle
we will have to use an infinite number of parameters to fully characterize the density.
One well-known nonparametric estimation is the histogram. The shape of the bars of the
histogram depends on the partition of the support. If the grid system on the support is too
fine, then each bin will have only a few observations. Despite small bias, the estimation
will suffer a large variance. On the other hand, if the grid system is too coarse, then each bin will be wide. It causes big bias, though the variance is small because each bin contains many observations. There is an bias-variance tradeoff. This tradeoff is the defining feature not only for nonparametric estimation but for all machine learning methods.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">&lt;-</span> <span class="m">200</span>

<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">3</span><span class="p">,</span> <span class="m">3</span><span class="p">))</span>
<span class="nf">par</span><span class="p">(</span><span class="n">mar</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">1</span><span class="p">))</span>

<span class="n">x_base</span> <span class="o">&lt;-</span> <span class="nf">seq</span><span class="p">(</span><span class="m">0.01</span><span class="p">,</span><span class="m">1</span><span class="p">,</span><span class="n">by</span> <span class="o">=</span> <span class="m">0.01</span><span class="p">)</span>
<span class="n">breaks_list</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">4</span><span class="p">,</span> <span class="m">12</span><span class="p">,</span> <span class="m">60</span><span class="p">)</span>

<span class="nf">for </span><span class="p">(</span><span class="n">ii</span> <span class="n">in</span> <span class="m">1</span><span class="o">:</span><span class="m">3</span><span class="p">){</span>
  <span class="n">x</span> <span class="o">&lt;-</span> <span class="nf">rbeta</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">2</span><span class="p">)</span> <span class="c1"># beta distribution</span>
  <span class="nf">for </span><span class="p">(</span> <span class="n">bb</span> <span class="n">in</span> <span class="n">breaks_list</span><span class="p">){</span>
    <span class="nf">hist</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">breaks</span> <span class="o">=</span> <span class="n">bb</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;&quot;</span><span class="p">,</span> <span class="n">freq</span> <span class="o">=</span> <span class="kc">FALSE</span><span class="p">,</span> <span class="n">ylim</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">3</span><span class="p">),</span><span class="n">xlim</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">))</span>
    <span class="nf">lines</span><span class="p">(</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">dbeta</span><span class="p">(</span> <span class="n">x_base</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">2</span><span class="p">),</span> <span class="n">x</span> <span class="o">=</span> <span class="n">x_base</span> <span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&quot;red&quot;</span> <span class="p">)</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>\begin{figure}
\centering
\begin{tikzpicture}[scale=1, transform shape] %size of the picture</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\draw[-&gt;, thick] (0,0) node[below left]{0} to (12,0) node[below]{bin size};
\draw[-&gt;, thick] (0,0) to (0,8);
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\draw [color = red, thick] (2,7) .. controls (6,2) and (10,5) .. (10.5,6.5) node[right]{MSE}; % MSE
\draw (1,0.1)     node[above]{bias$^2$}   parabola (11,5); % bias^2
\draw (11,1)    node[above]{variance}   parabola (1,7); % variance
</pre></div>
</div>
<p>\end{tikzpicture}
\caption{Bias-variance tradeoff}
\end{figure}</p>
<p>Another example of nonparametric estimation is  the conditional mean
<span class="math notranslate nohighlight">\(f(x) = E[y_i |x_i = x]\)</span> given a sample <span class="math notranslate nohighlight">\((y_i, x_i)\)</span>. This is what we encountered in the first lecture of graduate econometrics Econ5121A. We solve the minimization problem</p>
<div class="math notranslate nohighlight">
\[
\min_f E[ (y_i - f(x_i) )^2 ]
\]</div>
<p>In Econ5121A, we use the linear projection to approximate <span class="math notranslate nohighlight">\(f(x)\)</span>.
But the conditional mean is in general a nonlinear function.
If we do not know the underlying parametric estimation of
<span class="math notranslate nohighlight">\((y_i,x_i)\)</span>, estimating <span class="math notranslate nohighlight">\(f(x)\)</span> becomes a non-parametric problem.
In practice, the sample size <span class="math notranslate nohighlight">\(n\)</span> is always finite. The sample minimization problem is</p>
<div class="math notranslate nohighlight">
\[
\min_f \sum_{i=1}^n (y_i - f(x_i) )^2.
\]</div>
<p>We still have to restrict the class of functions that we search for the minimizer.
If we assume <span class="math notranslate nohighlight">\(f\)</span> is a continuous function, one way to estimate it is the kernel method based on density
estimation.</p>
<p>An alternative is to use a series expansion to approximate the function.
Series expansion generates many additive regressors whose coefficients will be
estimated. This is one way to “create” many variables on the right-hand side of a
linear regression.
For example, any bounded, continuous and differentiate function has a series
representation <span class="math notranslate nohighlight">\(f(x) = \sum_{k=0}^{\infty} \beta_k \cos (\frac{k}{2}\pi x )\)</span>. In finite sample,
we choose a finite <span class="math notranslate nohighlight">\(K\)</span>, usually much smaller than <span class="math notranslate nohighlight">\(n\)</span>, as a cut-off.
Asymptotically <span class="math notranslate nohighlight">\(K \to \infty\)</span> as <span class="math notranslate nohighlight">\(n \to \infty\)</span> so that</p>
<div class="math notranslate nohighlight">
\[
f_K(x) = \sum_{k=0}^{K} \beta_k \cos \left(\frac{k}{2}\pi x \right) \to f(x).
\]</div>
<p>Similar bias-variance tradeoff appears in this nonparametric regression.
If <span class="math notranslate nohighlight">\(K\)</span> is too big, <span class="math notranslate nohighlight">\(f\)</span> will be too flexible and it can achieve <span class="math notranslate nohighlight">\(100\%\)</span> of in-sample R-squared.
This is not useful for out-of-sample prediction. Such prediction will have large variance, but small bias.
On the other extreme, a very small <span class="math notranslate nohighlight">\(K\)</span> will make <span class="math notranslate nohighlight">\(f_K(x)\)</span> too rigid to approximate general nonlinear functions. It causes large bias but small variance.</p>
<p>The fundamental statistical mechanism that governs the performance is the bias-variance
tradeoff. Thus we need <em>regularization</em> to balance the two components in the mean-squared
error. Choosing the bandwidth is one way of regularization, choosing the terms of series
expansion is another way of regularization.</p>
<p>A third way of regularization is to specify a
sufficiently large <span class="math notranslate nohighlight">\(K\)</span>, and then add a penalty term to control the complexity of the additive series. The optimization problem is</p>
<div class="math notranslate nohighlight">
\[
\min_\beta \  \frac{1}{2n}  \sum_{i=1}^n \left(y_i - \sum_{k=0}^{K} \beta_k f_k(x_i) \right)^2
+ \lambda \sum_{k=0}^K \beta_k^2,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda\)</span> is the tuning parameter such that <span class="math notranslate nohighlight">\(\lambda \to 0\)</span> as <span class="math notranslate nohighlight">\(n\to \infty\)</span>, and
<span class="math notranslate nohighlight">\(f_k(x_i) = \cos \left(\frac{k}{2}\pi x_i \right)\)</span>. In compact notation, let <span class="math notranslate nohighlight">\(y=(y_1,\ldots,y_n)'\)</span> and
<span class="math notranslate nohighlight">\(X = (X_{ik} = f_k(x_i) )\)</span>, the above problem can be written as</p>
<div class="math notranslate nohighlight">
\[
(2n)^{-1} (Y-X\beta)'(Y-X\beta) + \lambda \Vert \beta \Vert_2 ^2,
\]</div>
<p>and this optimization has an explicit solution <span class="math notranslate nohighlight">\(\hat{\beta} = (X'X+\lambda I)^{-1} X'Y\)</span>. This is the <em>ridge regression</em> proposed in 1970’s.
This penalization scheme is very similar
to what we will discuss in the next section in variable selection.</p>
<p>The practical question is, given a regularization problem, how to choose the tuning
parameter? This is a difficult statistical problem with active research. The main
theoretical proposal is either using an <em>information criterion</em>
(for example, Akaike information criterion <span class="math notranslate nohighlight">\(\log\hat{\sigma}^2 + 2K\)</span> or Bayesian information criterion <span class="math notranslate nohighlight">\(\log\hat{\sigma}^2 + K\log n\)</span> ), or <em>cross validation</em>.</p>
</section>
<section id="data-splitting">
<h2>Data Splitting<a class="headerlink" href="#data-splitting" title="永久链接至标题">#</a></h2>
<!-- \begin{figure} -->
<!-- \centering -->
<!-- \begin{tikzpicture}[node distance = 10 mm, thick, scale=1, transform shape] -->
<!-- % Nodes -->
<!--   \node[ellipse, draw] -->
<!--     (a) [label=above left:$data$]{Training Data}; -->
<!--   \node[ellipse, draw, right = of a] -->
<!--     (b) {Validation Data}; -->
<!--   \node[ellipse, draw=red,fill=black!0, right = of b, text width=3cm, align=center] -->
<!--     (c) {(Out of Sample) Testing Data}; -->
<!--   \node[ellipse, draw=blue,fill=black!0, below right = 20 mm of a] -->
<!--     (d) {Fitted model} ; -->
<!--   \node[ellipse, draw=blue,fill=black!0, right= 10 mm of d, text width=3.5cm, align=center] -->
<!--     (e) {Best tuning Parameter (Model)}; -->
<!--   \node[ellipse, draw, left = 20 mm of d, loosely dashed] -->
<!--     (f) -->
<!--     [label = below: \textcolor{black!40}{Many Sets of Tuning Parameters}] -->
<!--     {Model}; -->
<!-- % Arrows -->
<!--     \draw [->, black] (a) -- (d); -->
<!--     \draw [->, blue] (d) -- (b); -->
<!--     \draw [->, blue] (b) -- (e); -->
<!--     \draw [->, blue] (e) -- (c); -->
<!--     \draw [->, black, loosely dashed] (f) -- (d); -->
<!-- % Caption     -->
<!--   \node[below = of d] { -->
<!--         \begin{tabular}{l} -->
<!--             $\bullet$ Data splitting can be done by cross validation \\ -->
<!--             $\bullet$ A data driven approach for feature selection -->
<!--         \end{tabular}}; -->
<!-- \end{tikzpicture} -->
<!-- \caption{Learning workflow} -->
<!-- \end{figure} -->
<p>The workflow of machine learning methods is quite different from econometrics. The main purpose is often prediction instead of interpretation.
They use some “off-the-shelf” generic learning methods, and
the models are measured by their performance in prediction.
In order to avoid overfitting it is essential to tune at least a few tuning parameters.</p>
<p>Most machine learning methods take an agnostic view about the DGP, and they explicitly
acknowledge model uncertainty. To address the issue of model selection (tuning parameter selection),
in a data rich environment we split the data into three parts. A <em>training dataset</em>
is used to fit the model parameter given the tuning parameters. A <em>validation dataset</em> is
used to compare the out-of-sample performance under different tuning parameters.
It helps decide a set of desirable tuning parameters. Ideally, the <em>testing sample</em> should be
kept by a third party away from the modeler. The testing sample is the final
judge of the relative merit of the fitted models.</p>
<p>The R package <code class="docutils literal notranslate"><span class="pre">caret</span></code> (Classification And REgression Training) provides a framework for many machine learning methods.
The function <a class="reference external" href="https://topepo.github.io/caret/data-splitting.html"><code class="docutils literal notranslate"><span class="pre">createDataPartition</span></code></a>
splits the sample for both cross-sectional data and time series.</p>
<section id="cross-validation">
<h3>Cross Validation<a class="headerlink" href="#cross-validation" title="永久链接至标题">#</a></h3>
<p>An <span class="math notranslate nohighlight">\(S\)</span>-fold cross validation partitions the dataset into <span class="math notranslate nohighlight">\(S\)</span> disjoint sections. In each iteration, it picks one of the sections as the validation sample and the other <span class="math notranslate nohighlight">\(S-1\)</span> sections as the training sample, and computes an out-of-sample goodness-of-fit measurement, for example <em>mean-squared prediction error</em> <span class="math notranslate nohighlight">\({n_v}^{-1} \sum_{i \in val} (y_i - \hat{y}_i)^2\)</span> where <span class="math notranslate nohighlight">\(val\)</span> is the validation set and <span class="math notranslate nohighlight">\(n_v\)</span> is its cardinality,  or <em>mean-absolute prediction error</em> <span class="math notranslate nohighlight">\({n_v}^{-1}\sum_{i \in val} |y_i - \hat{y}_i|\)</span>. Repeat this process for <span class="math notranslate nohighlight">\(S\)</span> times so that each of the <span class="math notranslate nohighlight">\(S\)</span> sections are treated as the validation sample, and average the goodness-of-fit measurement over the <span class="math notranslate nohighlight">\(S\)</span> sections to determined the best tuning parameter. If <span class="math notranslate nohighlight">\(S=n-1\)</span>, it is called <em>leave-one-out cross validation</em>, but it can be computationally too expensive when  <span class="math notranslate nohighlight">\(n\)</span> is big. Instead, in practice we can  <span class="math notranslate nohighlight">\(S=5\)</span> for 10, called 5-fold cross validation or 10-fold cross validation, respectively.</p>
<p>\begin{figure}
\centering
\includegraphics[width = 14cm]{graph/CV_Figure}
\caption{Rolling window time series cross validation}
\end{figure}</p>
<p>In time series context, cross validation must preserve the dependence structure. If the time series is stationary, we can partition the data into <span class="math notranslate nohighlight">\(S\)</span> consecutive blocks. If the purpose is ahead-of-time forecasting, then we can use nested CV. The figure shows a nested CV with fixed-length rolling window scheme, while the sub-training data can also be an extending rolling window.</p>
</section>
</section>
<section id="variable-selection-and-prediction">
<h2>Variable Selection and Prediction<a class="headerlink" href="#variable-selection-and-prediction" title="永久链接至标题">#</a></h2>
<p>In modern scientific analysis, the number of covariates <span class="math notranslate nohighlight">\(x_i\)</span> can be enormous.
In DNA microarray analysis, we look for association between a symptom and genes.
Theory in biology indicates that only a small handful of genes are involved,
but it does not pinpoint which ones are the culprits.
Variable selection is useful to identify the relevant genes, and then we can
think about how to edit the genes to prevent certain diseases and better people’s life.</p>
<p>Explanatory variables are abundant in some empirical economic examples.
For instance, a questionnaire from the <a class="reference external" href="https://discover.ukdataservice.ac.uk/series/?sn=2000028">UK Living Costs and Food Survey</a>, a survey
widely used for analysis of demand theory and family consumption,
consists of thousand of questions.
&#64;giannone2017economic <a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3031893">link</a> experiment variable selection methods in 6 widely used economic datasets with many predictors.</p>
<p><strong>Hazard of model selection</strong> To elaborate the distortion of test size when the <span class="math notranslate nohighlight">\(t\)</span> statistic is selected from two models in pursuit of significance.
$<span class="math notranslate nohighlight">\(
\begin{pmatrix}y\\
x_{1}\\
x_{2}
\end{pmatrix}\sim N\left(0,\begin{pmatrix}1 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; \sqrt{0.5}\\
0 &amp; \sqrt{0.5} &amp; 1
\end{pmatrix}\right)
\)</span><span class="math notranslate nohighlight">\(
Both \)</span>x_1<span class="math notranslate nohighlight">\( and \)</span>x_2<span class="math notranslate nohighlight">\( are independent of \)</span>y$. The test size dependens on the correlation between the two regressors.
If the test is conducted for a single model, the size is the pre-specified 10%.
If we try two models, the size is inflated to about 17%.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">&lt;-</span> <span class="m">100</span>
<span class="n">Rep</span> <span class="o">&lt;-</span> <span class="m">5000</span>

<span class="n">t_stat</span> <span class="o">&lt;-</span> <span class="nf">function</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">beta_hat</span> <span class="o">&lt;-</span> <span class="nf">sum</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="nf">sum</span><span class="p">(</span><span class="n">x</span><span class="o">^</span><span class="m">2</span><span class="p">)</span>
  <span class="n">e_hat</span> <span class="o">&lt;-</span> <span class="n">y</span> <span class="o">-</span> <span class="n">beta_hat</span> <span class="o">*</span> <span class="n">x</span>
  <span class="n">sigma2_hat</span> <span class="o">&lt;-</span> <span class="nf">var</span><span class="p">(</span><span class="n">e_hat</span><span class="p">)</span>
  <span class="n">t_stat</span> <span class="o">&lt;-</span> <span class="n">beta_hat</span> <span class="o">/</span> <span class="nf">sqrt</span><span class="p">(</span><span class="n">sigma2_hat</span> <span class="o">/</span> <span class="nf">sum</span><span class="p">(</span><span class="n">x</span><span class="o">^</span><span class="m">2</span><span class="p">))</span>
  <span class="nf">return</span><span class="p">(</span><span class="n">t_stat</span><span class="p">)</span>
<span class="p">}</span>

<span class="n">res</span> <span class="o">&lt;-</span> <span class="nf">matrix</span><span class="p">(</span><span class="kc">NA</span><span class="p">,</span> <span class="n">Rep</span><span class="p">,</span> <span class="m">2</span><span class="p">)</span>

<span class="nf">for </span><span class="p">(</span><span class="n">r</span> <span class="n">in</span> <span class="m">1</span><span class="o">:</span><span class="n">Rep</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">y</span> <span class="o">&lt;-</span> <span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
  <span class="n">x1</span> <span class="o">&lt;-</span> <span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
  <span class="n">x2</span> <span class="o">&lt;-</span> <span class="nf">sqrt</span><span class="p">(</span><span class="m">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">+</span> <span class="nf">sqrt</span><span class="p">(</span><span class="m">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

  <span class="n">res</span><span class="p">[</span><span class="n">r</span><span class="p">,</span> <span class="p">]</span> <span class="o">&lt;-</span> <span class="nf">c</span><span class="p">(</span><span class="nf">t_stat</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x1</span><span class="p">),</span> <span class="nf">t_stat</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x2</span><span class="p">))</span>
<span class="p">}</span>

<span class="nf">print</span><span class="p">(</span><span class="nf">mean</span><span class="p">(</span><span class="nf">apply</span><span class="p">(</span><span class="nf">abs</span><span class="p">(</span><span class="n">res</span><span class="p">),</span> <span class="m">1</span><span class="p">,</span> <span class="n">max</span><span class="p">)</span> <span class="o">&gt;</span> <span class="nf">qnorm</span><span class="p">(</span><span class="m">0.95</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<p>Conventionally, applied economists do not appreciate the problem of variable selection, even though they always select variables implicitly. They rely on their prior knowledge to
choose variables from a large number of potential candidates.
Recently years economists wake up from the long lasting negligence.
&#64;stock2012generalized are concerning about forecasting 143 US macroeconomic indicators.
They conduct a horse race of several variable selection methods.</p>
<p>The most well-known variable selection method in regression context is the least-absolute-shrinkage-and-selection-operator
(Lasso) [&#64;tibshirani1996regression].
Upon the usual OLS criterion function, Lasso penalizes the <span class="math notranslate nohighlight">\(L_1\)</span> norm of the coefficients.
The criterion function of Lasso is written as</p>
<div class="math notranslate nohighlight">
\[
(2n)^{-1} (Y-X\beta)'(Y-X\beta) + \lambda \Vert \beta \Vert_1
\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda \geq 0\)</span> is a tuning parameter. Unlike OLS or ridge regression, Lasso does not have a closed-form solution. Fortunately, it is an convex optimization so numerical optimization is fast and reliable for high-dimensional parameter.</p>
<p>In a wide range of values of <span class="math notranslate nohighlight">\(\lambda\)</span>,
Lasso can shrink some coefficients exactly to 0, which suggests that these variables are likely to be
irrelevant in the regression. This phenomenon is similar to “corner solution” that we solve utility maximization in microeconomics.</p>
<p>In terms of theoretical property, &#64;zou2006adaptive finds that Lasso cannot consistently
distinguish the relevant variables from the irrelevant ones.</p>
<p><img alt="lasso" src="_images/lasso_regression2.png" /></p>
<p><img alt="SCAD" src="_images/SCAD.png" /></p>
<p>Another successful variable selection method is smoothly-clipped-absolute-deviation (SCAD)
[&#64;fan2001variable]. Its criterion function is</p>
<div class="math notranslate nohighlight">
\[
(2n)^{-1} (Y-X\beta)'(Y-X\beta) + \sum_{j=1}^d \rho_{\lambda}( |\beta_j| )
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\rho_{\lambda}^{\prime} (\theta) = \lambda \left\{ 1\{\theta\leq \lambda \} +
\frac{(a\lambda - \theta)_+}{(a-1)\lambda} \cdot 1 \{\theta &gt; \lambda\} \right\}
\]</div>
<p>for some <span class="math notranslate nohighlight">\(a&gt;2\)</span> and <span class="math notranslate nohighlight">\(\theta&gt;0\)</span>. This is a non-convex function, and &#64;fan2001variable establish the so-called
<em>oracle property</em>. An estimator boasting the oracle property can achieve variable selection consistency and
(pointwise) asymptotic normality simultaneously.</p>
<p>The follow-up <em>adaptive Lasso</em> [&#64;zou2006adaptive] also enjoys the oracle property.
Adaptive Lasso is a two step scheme: 1. First run a Lasso or ridge regression and save the estimator <span class="math notranslate nohighlight">\(\hat{\beta}^{(1)}\)</span>. 2. Solve</p>
<div class="math notranslate nohighlight">
\[
(2n)^{-1} (Y-X\beta)'(Y-X\beta) + \lambda \sum_{j=1}^d  w_j |\beta_j|
\]</div>
<p>where <span class="math notranslate nohighlight">\(w_j = 1 \bigg/ \left|\hat{\beta}_j^{(1)}\right|^a\)</span> and <span class="math notranslate nohighlight">\(a\geq 1\)</span> is a constant. (Common choice is <span class="math notranslate nohighlight">\(a = 1\)</span> or 2).</p>
<p>In R,  <code class="docutils literal notranslate"><span class="pre">glmnet</span></code> or <code class="docutils literal notranslate"><span class="pre">LARS</span></code> implements Lasso, and <code class="docutils literal notranslate"><span class="pre">ncvreg</span></code> carries out SCAD. Adaptive Lasso
can be done by setting the weight via the argument <code class="docutils literal notranslate"><span class="pre">penalty.factor</span></code> in <code class="docutils literal notranslate"><span class="pre">glmnet</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">&lt;-</span> <span class="m">40</span>
<span class="n">p</span> <span class="o">&lt;-</span> <span class="m">50</span>
<span class="n">b0</span> <span class="o">&lt;-</span> <span class="nf">c</span><span class="p">(</span><span class="nf">rep</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="m">10</span><span class="p">),</span> <span class="nf">rep</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="n">p</span> <span class="o">-</span> <span class="m">10</span><span class="p">))</span>
<span class="n">x</span> <span class="o">&lt;-</span> <span class="nf">matrix</span><span class="p">(</span><span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">p</span><span class="p">),</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
<span class="n">y</span> <span class="o">&lt;-</span> <span class="n">x</span> <span class="o">%*%</span> <span class="n">b0</span> <span class="o">+</span> <span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="n">ols</span> <span class="o">&lt;-</span> <span class="n">MASS</span><span class="o">::</span><span class="nf">ginv</span><span class="p">(</span><span class="nf">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">%*%</span> <span class="n">x</span><span class="p">)</span> <span class="o">%*%</span> <span class="p">(</span><span class="nf">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">%*%</span> <span class="n">y</span><span class="p">)</span> <span class="c1"># OLS</span>
<span class="c1"># Implement Lasso by glmnet</span>
<span class="n">cv_lasso</span> <span class="o">&lt;-</span> <span class="n">glmnet</span><span class="o">::</span><span class="nf">cv.glmnet</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">lasso_result</span> <span class="o">&lt;-</span> <span class="n">glmnet</span><span class="o">::</span><span class="nf">glmnet</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lambda</span> <span class="o">=</span> <span class="n">cv_lasso</span><span class="o">$</span><span class="n">lambda.min</span><span class="p">)</span>

<span class="c1"># Get weights</span>
<span class="n">b_temp</span> <span class="o">&lt;-</span> <span class="nf">as.numeric</span><span class="p">(</span><span class="n">lasso_result</span><span class="o">$</span><span class="n">beta</span><span class="p">)</span>
<span class="n">b_temp</span><span class="p">[</span><span class="n">b_temp</span> <span class="o">==</span> <span class="m">0</span><span class="p">]</span> <span class="o">&lt;-</span> <span class="m">1e-8</span>
<span class="n">w</span> <span class="o">&lt;-</span> <span class="m">1</span> <span class="o">/</span> <span class="nf">abs</span><span class="p">(</span><span class="n">b_temp</span><span class="p">)</span> <span class="c1"># Let gamma = 1</span>

<span class="c1"># Implement Adaptive Lasso by glmnet</span>
<span class="n">cv_alasso</span> <span class="o">&lt;-</span> <span class="n">glmnet</span><span class="o">::</span><span class="nf">cv.glmnet</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">penalty.factor</span> <span class="o">=</span> <span class="n">w</span><span class="p">)</span>
<span class="n">alasso_result</span> <span class="o">&lt;-</span>
  <span class="n">glmnet</span><span class="o">::</span><span class="nf">glmnet</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">penalty.factor</span> <span class="o">=</span> <span class="n">w</span><span class="p">,</span> <span class="n">lambda</span> <span class="o">=</span> <span class="n">cv_alasso</span><span class="o">$</span><span class="n">lambda.min</span><span class="p">)</span>

<span class="nf">plot</span><span class="p">(</span><span class="n">b0</span><span class="p">,</span> <span class="n">ylim</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">-0.8</span><span class="p">,</span> <span class="m">1.5</span><span class="p">),</span> <span class="n">pch</span> <span class="o">=</span> <span class="m">4</span><span class="p">,</span> <span class="n">xlab</span> <span class="o">=</span> <span class="s">&quot;&quot;</span><span class="p">,</span> <span class="n">ylab</span> <span class="o">=</span> <span class="s">&quot;coefficient&quot;</span><span class="p">)</span>
<span class="nf">points</span><span class="p">(</span><span class="n">lasso_result</span><span class="o">$</span><span class="n">beta</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&quot;red&quot;</span><span class="p">,</span> <span class="n">pch</span> <span class="o">=</span> <span class="m">6</span><span class="p">)</span>
<span class="nf">points</span><span class="p">(</span><span class="n">alasso_result</span><span class="o">$</span><span class="n">beta</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&quot;blue&quot;</span><span class="p">,</span> <span class="n">pch</span> <span class="o">=</span> <span class="m">5</span><span class="p">)</span>
<span class="nf">points</span><span class="p">(</span><span class="n">ols</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&quot;green&quot;</span><span class="p">,</span> <span class="n">pch</span> <span class="o">=</span> <span class="m">3</span><span class="p">)</span>

<span class="c1"># out of sample prediction</span>
<span class="n">x_new</span> <span class="o">&lt;-</span> <span class="nf">matrix</span><span class="p">(</span><span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">p</span><span class="p">),</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
<span class="n">y_new</span> <span class="o">&lt;-</span> <span class="n">x_new</span> <span class="o">%*%</span> <span class="n">b0</span> <span class="o">+</span> <span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">lasso_msfe</span> <span class="o">&lt;-</span> <span class="p">(</span><span class="n">y_new</span> <span class="o">-</span> <span class="nf">predict</span><span class="p">(</span><span class="n">lasso_result</span><span class="p">,</span> <span class="n">newx</span> <span class="o">=</span> <span class="n">x_new</span><span class="p">))</span> <span class="o">%&gt;%</span> <span class="nf">var</span><span class="p">()</span>
<span class="n">alasso_msfe</span> <span class="o">&lt;-</span> <span class="p">(</span><span class="n">y_new</span> <span class="o">-</span> <span class="nf">predict</span><span class="p">(</span><span class="n">alasso_result</span><span class="p">,</span> <span class="n">newx</span> <span class="o">=</span> <span class="n">x_new</span><span class="p">))</span> <span class="o">%&gt;%</span> <span class="nf">var</span><span class="p">()</span>
<span class="n">ols_msfe</span> <span class="o">&lt;-</span> <span class="p">(</span><span class="n">y_new</span> <span class="o">-</span> <span class="n">x_new</span> <span class="o">%*%</span> <span class="n">ols</span><span class="p">)</span> <span class="o">%&gt;%</span> <span class="nf">var</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="n">lasso_msfe</span><span class="p">,</span> <span class="n">alasso_msfe</span><span class="p">,</span> <span class="n">ols_msfe</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>We can DIY Lasso by <code class="docutils literal notranslate"><span class="pre">CVXR</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">CVXR</span><span class="p">)</span>

<span class="n">lambda</span> <span class="o">&lt;-</span> <span class="m">2</span> <span class="o">*</span> <span class="n">cv_lasso</span><span class="o">$</span><span class="n">lambda.min</span> <span class="c1"># tuning parameter</span>

<span class="c1"># CVXR for Lasso</span>
<span class="n">beta_cvxr</span> <span class="o">&lt;-</span> <span class="nf">Variable</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
<span class="n">obj</span> <span class="o">&lt;-</span> <span class="nf">sum_squares</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">x</span> <span class="o">%*%</span> <span class="n">beta_cvxr</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="m">2</span> <span class="o">*</span> <span class="n">n</span><span class="p">)</span> <span class="o">+</span> <span class="n">lambda</span> <span class="o">*</span> <span class="nf">p_norm</span><span class="p">(</span><span class="n">beta_cvxr</span><span class="p">,</span> <span class="m">1</span><span class="p">)</span>
<span class="n">prob</span> <span class="o">&lt;-</span> <span class="nf">Problem</span><span class="p">(</span><span class="nf">Minimize</span><span class="p">(</span><span class="n">obj</span><span class="p">))</span>
<span class="n">lasso_cvxr</span> <span class="o">&lt;-</span> <span class="nf">solve</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
<span class="n">beta_cvxr_hat</span> <span class="o">&lt;-</span> <span class="n">lasso_cvxr</span><span class="o">$</span><span class="nf">getValue</span><span class="p">(</span><span class="n">beta_cvxr</span><span class="p">)</span> <span class="o">%&gt;%</span> <span class="nf">as.vector</span><span class="p">()</span> <span class="o">%&gt;%</span> <span class="nf">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>More methods are available if prediction of the response variables is the sole purpose of the regression.
An intuitive one is called <em>stagewise forward selection</em>.
We start from an empty model. Given many candidate <span class="math notranslate nohighlight">\(x_j\)</span>, in each round we add the regressor that can
produce the biggest <span class="math notranslate nohighlight">\(R^2\)</span>. This method is similar to the idea of <span class="math notranslate nohighlight">\(L_2\)</span> componentwise boosting,
which does not adjust the coefficients fitted earlier.</p>
</section>
<section id="shrinkage-estimation-in-econometrics">
<h2>Shrinkage Estimation in Econometrics<a class="headerlink" href="#shrinkage-estimation-in-econometrics" title="永久链接至标题">#</a></h2>
<ul class="simple">
<li><p>&#64;su2016identifying: use shrinkage estimation for classification</p></li>
<li><p>&#64;shi2016estimation: convergence rate of GMM Lasso</p></li>
<li><p>&#64;lee2018: Lasso and adaptive Lasso in predictive regression</p></li>
<li><p>&#64;shi2019forward: forward selection</p></li>
<li><p>&#64;shi2020high: latent group in forecast combination</p></li>
</ul>
</section>
<section id="empirical-applications">
<h2>Empirical Applications<a class="headerlink" href="#empirical-applications" title="永久链接至标题">#</a></h2>
<ul class="simple">
<li><p>&#64;lehrer2017box: movie box office</p></li>
<li><p>&#64;feng2019taming: factor zoo, compare machine learning methods</p></li>
<li><p>&#64;chinco2017sparse: financial market, Lasso prediction</p></li>
</ul>
</section>
<section id="reading">
<h2>Reading<a class="headerlink" href="#reading" title="永久链接至标题">#</a></h2>
<ul class="simple">
<li><p>Efron and Hastie: Ch. 16</p></li>
<li><p>&#64;athey2018impact</p></li>
</ul>
</section>
<section id="appendix">
<h2>Appendix<a class="headerlink" href="#appendix" title="永久链接至标题">#</a></h2>
<p>Suppose <span class="math notranslate nohighlight">\(y_i = x_i' \beta_0 + e_i\)</span>, where <span class="math notranslate nohighlight">\(e_i\)</span> is independent of <span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\(\mathrm{var}[e_i] = \sigma^2\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\min_{\beta} E[ (y_i - x_i' \beta)^2 ] = E[ (y_i - x_i' \beta_0)^2 ] = E[ e_i^2 ] = \sigma^2.
\]</div>
<p>This is the minimal error that can be achieved in the population.</p>
<p>In reality, we have a sample <span class="math notranslate nohighlight">\((y_i, x_i)\)</span> of <span class="math notranslate nohighlight">\(n\)</span> observations, and we estimate <span class="math notranslate nohighlight">\(\beta\)</span> by the OLS estimator <span class="math notranslate nohighlight">\(\hat{\beta} = (X'X)^{-1}X'y\)</span>.
The expectation of the SSR is</p>
<div class="math notranslate nohighlight">
\[
E\left[ \frac{1}{n} \sum_{i=1}^n (y_i - x_i' \hat{\beta})^2  \right]
= \frac{1}{n}  E\left[ e'(I_n - X(X'X)^{-1}X )e  \right]=  \frac{\sigma^2}{n}(n-p) = \sigma^2\left( 1 - \frac{p}{n} \right)
&lt; \sigma^2
\]</div>
<p>Asymptotically, if <span class="math notranslate nohighlight">\(p/n \to 0\)</span>, the two risks converge. Otherwise if <span class="math notranslate nohighlight">\(p/n \to c\)</span>, the expected SSR is strictly
smaller than the minimal population risk. The model is overfitted.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="永久链接至标题">#</a></h2>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            kernelName: "ir",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By 史震涛 Shi Zhentao<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>